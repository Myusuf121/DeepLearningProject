# DeepLearningProject
This is a semester project of deep learning course. We applied transformers models on NLP domain (NER) and reported results.

# Transformer Models Implementation
This repository contains Jupyter Notebook files implementing various Transformer models for natural language processing tasks using libraries such as PyTorch and Hugging Face Transformers.

## Overview
The Transformer architecture has revolutionized the field of natural language processing (NLP) with its ability to capture long-range dependencies and learn contextual representations effectively. In this repository, we provide implementations of popular Transformer models such as BERT, GPT, and T5, along with examples demonstrating their usage for tasks like text classification, language generation, and more.

## Repository Structure
- **Notebooks**: This directory contains Jupyter Notebook files (.ipynb) with implementations of Transformer models and examples of how to use them for different NLP tasks.
  
- **Data**: Here, you can find sample datasets used in the notebooks for training and evaluation purposes.

## Requirements
To run the notebooks in this repository, you need to have the following installed:

- Python 3.x
- Jupyter Notebook
- PyTorch
- Hugging Face Transformers

You can install these dependencies using pip:

```bash
pip install torch
pip install transformers
```

## Usage
1. Clone this repository to your local machine:

```bash
git clone https://github.com/your-username/transformer-models.git
```

2. Navigate to the cloned directory:

```bash
cd transformer-models
```

3. Launch Jupyter Notebook:

```bash
jupyter notebook
```

4. Open the desired notebook from the `Notebooks` directory and run the cells to see the implementation and results.
5. 
## Contributing
Contributions are welcome! If you have suggestions, bug reports, or want to add new features, feel free to open an issue or submit a pull request.
